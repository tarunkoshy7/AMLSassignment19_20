{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import lab2_landmarks as l2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to extract the features from each of the images and split them into training, validation, and test sets. Feature extraction is done using dlib, which I called from the lab2_landmarks file given in lab 2 of this course. Once the features had been extracted, I used scikit-learn to split the dataset into a 60-20-20 train-val-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    X, y = l2.extract_features_labels()\n",
    "    #Y = np.array([y, -(y - 1)]).T\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "    X_train_final = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n",
    "    X_val_final = X_val.reshape(X_val.shape[0],X_val.shape[1]*X_val.shape[2])\n",
    "    X_test_final = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "    return X_train_final, y_train, X_val_final, y_val, X_test_final, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLR(training_images, training_labels, test_images, test_labels, lambd):\n",
    "    classifier = linear_model.LogisticRegression(C=lambd, solver='liblinear', max_iter=100)\n",
    "    classifier.fit(training_images, training_labels)\n",
    "    pred = classifier.predict(test_images)\n",
    "    \n",
    "    #accuracy = accuracy_score(test_labels, pred)\n",
    "    #print(\"Accuracy:\", accuracy_score(test_labels, pred))\n",
    "    #print(pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(X_train, y_train, X_val, y_val):\n",
    "    lambda_vec = np.array([0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n",
    "    \n",
    "    error_train = np.zeros(len(lambda_vec))\n",
    "    error_val = np.zeros(len(lambda_vec))\n",
    "    \n",
    "    for i in range(len(lambda_vec)):\n",
    "        lambd = lambda_vec[i]\n",
    "        y_train_pred = trainLR(X_train, y_train, X_train, y_train, lambd)\n",
    "        y_val_pred = trainLR(X_train, y_train, X_val, y_val, lambd)\n",
    "        \n",
    "        error_train[i] = log_loss(y_train, y_train_pred)\n",
    "        error_val[i] = log_loss(y_val, y_val_pred)\n",
    "\n",
    "    print('Training Error:', error_train)\n",
    "    print('Validation Error:', error_val)\n",
    "\n",
    "    minErrorIndex = np.argmin(error_val)\n",
    "    minError = np.amin(error_val)\n",
    "    print('Min Validation Error Value:', minError)\n",
    "    print('Ideal Lambda:', lambda_vec[minErrorIndex])\n",
    "    \n",
    "    plt.plot(lambda_vec, error_train, label='Train')\n",
    "    plt.plot(lambda_vec, error_val, label='Cross Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('lambda')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Validation Curve')\n",
    "    plt.show()    \n",
    "        \n",
    "    return lambda_vec[minErrorIndex], minError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X_train, y_train, X_val, y_val, lambd):\n",
    "        m = X_train.shape[0]\n",
    "        error_train = np.zeros(int(m/10))\n",
    "        error_val = np.zeros(int(m/10))\n",
    "        \n",
    "        for i in range(10, m, 10):\n",
    "            y_train_pred = trainLR(X_train[0:i], y_train[0:i], X_train[0:i], y_train[0:i], lambd)\n",
    "            y_val_pred = trainLR(X_train[0:i], y_train[0:i], X_val, y_val, lambd)\n",
    "            \n",
    "            error_train[int((i-10)/10)] = log_loss(y_train[0:i], y_train_pred)\n",
    "            error_val[int((i-10)/10)] = log_loss(y_val, y_val_pred)\n",
    "            \n",
    "        plt.plot(np.arange(10, m, 10), error_train, label='Train')\n",
    "        plt.plot(np.arange(10, m, 10), error_val, label='Cross Validation')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Number of Training Examples')\n",
    "        plt.ylabel('Error')\n",
    "        plt.title('Learning Curve')\n",
    "        plt.show()\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaIdeal = crossValidation(X_train, y_train, X_val, y_val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(X_train, y_train, X_val, y_val, lambdaIdeal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=trainLR(X_train, y_train, X_train, y_train, lambdaIdeal)\n",
    "print('Accuracy:', accuracy_score(y_train, pred_train))\n",
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val=trainLR(X_train, y_train, X_val, y_val, lambdaIdeal)\n",
    "print('Accuracy:', accuracy_score(y_val, pred_val))\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=trainLR(X_train, y_train, X_test, y_test, lambdaIdeal)\n",
    "print('Accuracy:', accuracy_score(y_test, pred_test))\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
